# -*- coding: utf-8 -*-
"""KTULLMTRAININGCODE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BiH3tvM7CGoPMzOg9SJ_vjrApQXW3PhZ

IMPORT UNSLOTH AND OTHER DEPENDENCIES
"""

!pip install unsloth
!pip install --upgrade --no-deps "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps "xformers==0.0.25+814915d.d20230118" -f https://github.com/facebookresearch/xformers/releases/download/v0.0.25%2B814915d/wheels

"""IMPORTING THE MODEL"""

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)

"""IMPORTING DATASET"""

!pip install datasets

from datasets import load_dataset

dataset = load_dataset("ClumsyAahDeveloper/sample003",split = "train",
)
print(dataset.column_names)
print(dataset)

import pandas as pd

df = dataset.to_pandas()

pd.set_option("display.max_rows",None)
pd.set_option("display.max_columns",None)

df

from huggingface_hub import login

login()

"""FORMATTING THE DATASET"""

from unsloth import to_sharegpt
dataset = to_sharegpt(
    dataset,
    merged_prompt = "{Question}[[\nYour input is:\n{Input}]]",
    output_column_name = "Output",
    conversation_extension = 3,
)

from unsloth import standardize_sharegpt

dataset = standardize_sharegpt(dataset)

"""PUTTING THE CHAT TEMPLATE"""

chat_template = """Below are some instructions that describe some tasks. Write responses that appropriately completes the request.

### Instruction:
{INPUT}

### Response:
{OUTPUT}"""

from unsloth import apply_chat_template
dataset = apply_chat_template(
    dataset,
    tokenizer = tokenizer,
    chat_template = chat_template,
)

import pandas as pd

def print_dataset(dataset):
    """Prints the dataset in a readable format."""
    if isinstance(dataset, pd.DataFrame):
        with pd.option_context('display.max_rows', None, 'display.max_columns', None):
            print(dataset)
    else:
        try:
            print(dataset)
        except TypeError:
            print("Dataset is not printable directly. Try converting to DataFrame first.")


print_dataset(dataset)

!pip install trl

"""SETTING TRAINING PARAMETERS"""

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
trainer = SFTTrainer(
    model=model,
    tokenizer = tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        num_train_epochs = 1,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "wandb",
    ),

)

"""TRAINING THE MODEL"""

!pip install wandb

import wandb

wandb.init(project="my-sft-project",name="my-training-run")

trainer_stats = trainer.train()

wandb.finish()

"""CREATING AN INFERENCE"""

FastLanguageModel.for_inference(model)
messages = [
    {"role": "user", "content": "indian prime minister"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)

"""SAVING THE FILE LOCALLY"""

model.save_pretrained("lora_model")
tokenizer.save_pretrained("lora_model")

"""PUSHING IT TO THE HUGGINGFACE REPOSITORY"""

model.push_to_hub("ClumsyAahDeveloper/lora_model",token = "")
tokenizer.push_to_hub("ClumsyAahDeveloper/lora_model",token = "")

!du -sh lora_model

"""RUNNING THE MODEL"""

if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model",
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model)
pass

messages = [
    {"role": "user", "content": "Who is Haridev?"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)

"""SAVING IT TO OLLAMA"""

!curl -fsSL https://ollama.com/install.sh | sh

if True: model.save_pretrained_gguf("model", tokenizer,)
if False: model.push_to_hub_gguf("ClumsyAahDeveloper/model", tokenizer, token = "")


if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf("ClumsyAahDeveloper/model", tokenizer, quantization_method = "f16", token = "")

if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf("ClumsyAahDeveloper/model", tokenizer, quantization_method = "q4_k_m", token = "")

if False:
    model.push_to_hub_gguf(
        "ClumsyAahDeveloper/model",
        tokenizer,
        quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
        token = "",
    )

import subprocess
subprocess.Popen(["ollama", "serve"])
import time
time.sleep(3)

print(tokenizer._ollama_modelfile)

!ollama create unsloth_model -f ./model/Modelfile

!curl http://localhost:11434/api/chat -d '{ \
    "model": "unsloth_model", \
    "messages": [ \
        { "role": "user", "content": "Who is Haridev?" } \
    ] \
    }'

"""TILL HERE COMPULSORY"""

# Evaluate the model
results = trainer.evaluate(eval_dataset=ClumsyAahDeveloper/sample003)
print(results)
